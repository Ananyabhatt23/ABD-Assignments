{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "hEptZEAaC88k",
        "VNG7CyRoDHFF",
        "Qe0DSJiqKNBG",
        "87dKR3wqL2EG",
        "M_aPedNINpxN",
        "NOL3F8PNSsal",
        "3kW4na6iS3pj",
        "sRsQFdYneNRi",
        "B0KUCMFbeSHG",
        "rlJOUuexeWol",
        "Uy2e5AYseZk4",
        "HyHtUEbYelye",
        "8BWfmv80H80_",
        "a-zpQegyuXbG"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ananyabhatt23/ABD-Assignments/blob/main/SparkAssignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwEiO7d45E9E",
        "outputId": "5a4ffbe8-dbbb-4ff5-d3ee-16333f59f6b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425344 sha256=a5ccaf3700b0e66e3f8e875815c02c33976d87812b420abc1a5e9bba22e25928\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ],
      "source": [
        "pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('Spark Assignment').getOrCreate()"
      ],
      "metadata": {
        "id": "gj6cp46p58D4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q1\n",
        "Create RDDs in three different ways."
      ],
      "metadata": {
        "id": "moQFJNO7Qz_W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# By PARALLELIZE\n",
        "parRDD = spark.sparkContext.parallelize([\"The\",\"Parallelize\",\"Method\",\"RDD\"])\n",
        "\n",
        "# By Loading Data from External Storage\n",
        "externalRDD = spark.sparkContext.textFile(\"/content/MYDRIVE/MyDrive/ABD Spark/Template.txt\")\n",
        "\n",
        "# By applying operations on existing data (map(), flatMap(), reduceByKey(), filter())\n",
        "externalRDD.map(lambda d:(d.split(' '),1)).collect()"
      ],
      "metadata": {
        "id": "Gfkvy_eaQ2Dg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q2\n",
        " Read a text file and count the number of words in the file using RDD operations."
      ],
      "metadata": {
        "id": "hEptZEAaC88k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd.flatMap(lambda d: (d.split(' ')[2],d.split(' ')[3])).count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BIqB4JT703b",
        "outputId": "2cbb0dba-af4a-44f9-8568-7aa01d1b1ca1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "380622"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q3\n",
        "Write a program to find the word frequency in a given file."
      ],
      "metadata": {
        "id": "VNG7CyRoDHFF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd.flatMap(lambda d: d.split('\\t')).collect()\n",
        "rdd.flatMap(lambda d: d.split('\\t')).map(lambda d: (d.split(' ')[2],d.split(' ')[3])).collect()\n",
        "rdd.flatMap(lambda d: d.split('\\t')).map(lambda d: (d.split(' ')[3])).distinct().collect()"
      ],
      "metadata": {
        "id": "uCEP3UYzBhi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdd.flatMap(lambda d: d.split('\\t')).map(lambda d: (d.split(' ')[2],1)).reduceByKey(lambda a,b: a+b).collect()"
      ],
      "metadata": {
        "id": "P9Cr-4QLKGfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q4\n",
        "Write a program to convert all words in a file to uppercase."
      ],
      "metadata": {
        "id": "Qe0DSJiqKNBG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd.flatMap(lambda d: d.split('\\t')).map(lambda d: (d.split(' ')[2].upper(),d.split(' ')[3].upper())).collect()"
      ],
      "metadata": {
        "id": "i4DFfd-iKS3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q5\n",
        "Write a program to convert all words in a file to lowercase."
      ],
      "metadata": {
        "id": "87dKR3wqL2EG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd.flatMap(lambda d: d.split('\\t')).map(lambda d: (d.split(' ')[2].lower(),d.split(' ')[3].lower())).distinct().collect()"
      ],
      "metadata": {
        "id": "oNyPyYCzL1pO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q6\n",
        " Write  a  program  to  capitalize  first  letter  of  each  words  in  file  (use  string capitalize() method)."
      ],
      "metadata": {
        "id": "M_aPedNINpxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = spark.sparkContext.textFile('/content/ananya in pariss..txt')\n",
        "rdd.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6Bd2tjoOLAr",
        "outputId": "50d48d54-586e-41de-861d-b57d9a458158"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ananya in pariss.',\n",
              " 'ananya in pariss.',\n",
              " 'ananya in pariss.',\n",
              " 'ananya in pariss.']"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd.flatMap(lambda d: d.split(' ')).map(lambda d: d.capitalize()).collect()"
      ],
      "metadata": {
        "id": "a5Y1deecPmju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q7\n",
        "Find the number of occurrence of a word in a given file."
      ],
      "metadata": {
        "id": "NOL3F8PNSsal"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGPHnoc5SoN9",
        "outputId": "8a83f92b-3f1c-4439-9bd4-4916c16f6b43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q8\n",
        "Select only the sentences containing given word from a text file."
      ],
      "metadata": {
        "id": "3kW4na6iS3pj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word = spark.sparkContext.textFile('/content/drive/MyDrive/Colab Notebooks/ABD/DataSet/sampleTemp.txt')\n",
        "findword = 'Achalpur'\n",
        "word.map(lambda d: (d.split(' '))).filter(lambda d: (d[2] == findword)).collect()"
      ],
      "metadata": {
        "id": "7kGkoL_vMi6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q9\n",
        "Find the longest length of word from given set of words."
      ],
      "metadata": {
        "id": "sRsQFdYneNRi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word.map(lambda d: d.split(' ')[2]).max()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Zz-6dHW0Q8xN",
        "outputId": "2c778d53-3a67-4f98-a0be-e5294e0d781d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Yelahanka'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q10\n",
        " Map the  Registration  numbers to corresponding  branch. 58000 series  BDA,57000  series AIML, 38000 series VLSI, 39000 series ES, and47000 series CDC.Given registration number, generate a key-value pair of Registration Number and Corresponding Branch."
      ],
      "metadata": {
        "id": "B0KUCMFbeSHG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "regRdd = spark.sparkContext.parallelize([58023,47023,38023,39023,57023])\n",
        "def branchFinding(num):\n",
        "  if num >=58001 and num <=59000:\n",
        "    return \"BDA\"\n",
        "  elif num >=57001 and num <=58000:\n",
        "    return \"AIML\"\n",
        "  elif num >=38001 and num <= 39000:\n",
        "    return \"VLSI\"\n",
        "  elif num >=39001 and num <= 40000:\n",
        "    return \"ES\"\n",
        "  elif num >= 47001 and num <= 48000:\n",
        "    return \"CDC\"\n",
        "  else:\n",
        "    return \"Unknown Registration Number\"\n",
        "\n",
        "regRdd.map(lambda d:(d,branchFinding(d))).collect()"
      ],
      "metadata": {
        "id": "cnSzQJG_SgcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q11\n",
        "Text file contain numbers. Numbers are separated by one white space. There is no order to store the numbers. One line may contain one or more numbers. Find the maximum, minimum, sum and mean of numbers."
      ],
      "metadata": {
        "id": "rlJOUuexeWol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "textfile = spark.sparkContext.textFile('/content/drive/MyDrive/Colab Notebooks/ABD/DataSet/Numbers.txt')\n",
        "textfile.collect()"
      ],
      "metadata": {
        "id": "TwBXjm8B1JSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "textfile.flatMap(lambda d: d.split(' ')).map(lambda d: int(d)).max()\n",
        "textfile.flatMap(lambda d: d.split(' ')).map(lambda d: int(d)).min()\n",
        "textfile.flatMap(lambda d: d.split(' ')).map(lambda d: int(d)).mean()\n",
        "textfile.flatMap(lambda d: d.split(' ')).map(lambda d: int(d)).sum()"
      ],
      "metadata": {
        "id": "DotkV3PS2iVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q12\n",
        "A text file(citizen.txt)contains data about citizens of country. Fields(information in file) are Name, dob, Phone, email and state name. Another file contains mapping of state names to state code like  Karnataka  is  codes  as  KA,TamilNadu  as  TN,  Kerala  KL  etc.  Compress  the citizen.txt file  by changing full state name to state code."
      ],
      "metadata": {
        "id": "Uy2e5AYseZk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/MYDRIVE/MyDrive/ABD Spark/Citizens.txt\",'w') as info:\n",
        "    info.write('abc 06-10-1999 1234567890 abc@gmail.com Karnataka \\nxyz 23-11-2000 2345678901 xyz@gmail.com TamilNadu \\ndef 29-02-1995 3456789012 def@gmail.com Goa')\n",
        "    info.close()\n",
        "\n",
        "print(\"Original File: \\n\")\n",
        "print(spark.sparkContext.textFile(\"/content/MYDRIVE/MyDrive/ABD Spark/Citizens.txt\").collect())\n",
        "\n",
        "stateCode = spark.sparkContext.textFile(\"/content/MYDRIVE/MyDrive/ABD Spark/State Code.txt\")\n",
        "\n",
        "def compressingFunction(state):\n",
        "  with open(\"/content/MYDRIVE/MyDrive/ABD Spark/Citizens.txt\",'r') as citizen:\n",
        "    cont = citizen.read()\n",
        "    cont  = cont.replace(state[0],state[1])\n",
        "    citizen.close()\n",
        "  with open (\"/content/MYDRIVE/MyDrive/ABD Spark/Citizens.txt\",'w') as citizen_new:\n",
        "    citizen_new.write(cont)\n",
        "    citizen_new.close()\n",
        "\n",
        "stateInfo = stateCode.flatMap(lambda data:data.split('\\n')).map(lambda d:[i for i in d.split(\" \")]).collect()\n",
        "for i in stateInfo:\n",
        "  compressingFunction(i)\n",
        "\n",
        "print(\"After Encoding: \\n\")\n",
        "spark.sparkContext.textFile(\"/content/MYDRIVE/MyDrive/ABD Spark/Citizens.txt\").collect()"
      ],
      "metadata": {
        "id": "eD67ZKGuedS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Question 1\n",
        " Create dataset (text file) with fields like ‘Student Name’, ‘Institute’, ‘Program Name’, and ‘Gender’ and solve following questions.\n",
        "\n",
        "\n",
        "1.   Compute number of students from each Institute.\n",
        "2.   Number of students enrolled to any program.\n",
        "3.   Number of ‘boy’ and ‘girl’ students.\n",
        "4.   Number of ‘boy’ and ‘girl’ students from selected Institute.\n"
      ],
      "metadata": {
        "id": "HyHtUEbYelye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = spark.sparkContext.textFile('/content/students.txt')\n",
        "rdd.collect()"
      ],
      "metadata": {
        "id": "fpIW1mbGfJNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdd.map(lambda d: (d.split(' ')[1],1)).reduceByKey(lambda a,b: a+b).collect()"
      ],
      "metadata": {
        "id": "NHDbGThugyrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdd.map(lambda d: (d.split(' ')[2],1)).reduceByKey(lambda a,b: a+b).collect()"
      ],
      "metadata": {
        "id": "vSCprRorkHFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdd.map(lambda d: (d.split(' ')[3],1)).reduceByKey(lambda a,b: a+b).collect()"
      ],
      "metadata": {
        "id": "bbrw344ok0y_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdd.map(lambda d: (d.split(' ')[1],d.split(' ')[3]=='F')).reduceByKey(lambda a,b: a+b).collect()\n"
      ],
      "metadata": {
        "id": "rHi8t3Armn6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdd.map(lambda d: (d.split(' ')[1],d.split(' ')[3]=='M')).reduceByKey(lambda a,b: a+b).collect()"
      ],
      "metadata": {
        "id": "ao32_DUGoBEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Question 2\n",
        "Dataset: Temperature of Indian Cities. Fields of dataset are Date, Average Temperature, City, Country, Latitude and Longitude(Use dataset attached to MapReduce assignment). Solve following questions\n",
        "\n",
        "1.   Find maximum and minimum temperature of all cities from the given dataset\n",
        "2.   Count number of data point for each city.\n",
        "3.   Find the maximum and minimum temperature for city Bangalorefrom the\n",
        "    given dataset.\n",
        "4.   Find the maximum and minimum temperature for any given city from the   given dataset.City name should be passed through command line argument.\n",
        "\n"
      ],
      "metadata": {
        "id": "8BWfmv80H80_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tempRdd = spark.sparkContext.textFile(\"/content/MYDRIVE/MyDrive/ABD Spark/Temperature Dataset.txt\")\n",
        "tempRdd.collect()"
      ],
      "metadata": {
        "id": "fkCX6HwaPgx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tempRdd.map(lambda d:(d.split(' ')[1],1)).max()\n",
        "tempRdd.map(lambda d:(d.split(' ')[1],1)).min()"
      ],
      "metadata": {
        "id": "LA3kYpqOPn1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tempRdd.map(lambda d:(d.split(' ')[2],1)).reduceByKey(lambda a,b:a+b).collect()"
      ],
      "metadata": {
        "id": "Jpp_a9ioPquD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tempRdd.map(lambda d:(d.split(' ')[1],d.split(' ')[2])).filter(lambda d:d[1]==\"Banglore\").max()\n",
        "tempRdd.map(lambda d:(d.split(' ')[1],d.split(' ')[2])).filter(lambda d:d[1]==\"Banglore\").min()"
      ],
      "metadata": {
        "id": "_S8w78SzPtIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Maximum and Minimum in Shivamogga\n",
        "terminal_args = ['jupyter notebook', 'Temperature.ipynb' ,'Shivamogga']\n",
        "tempRdd.map(lambda d:(d.split(' ')[1],d.split(' ')[2])).filter(lambda d:d[1]==terminal_args[2]).max()\n",
        "tempRdd.map(lambda d:(d.split(' ')[1],d.split(' ')[2])).filter(lambda d:d[1]==terminal_args[2]).min()\n",
        "\n",
        "# Maximum and Minimum in Banglore\n",
        "terminal_args = ['jupyter notebook', 'Temperature.ipynb' ,'Banglore']\n",
        "tempRdd.map(lambda d:(d.split(' ')[1],d.split(' ')[2])).filter(lambda d:d[1]==terminal_args[2]).max()\n",
        "tempRdd.map(lambda d:(d.split(' ')[1],d.split(' ')[2])).filter(lambda d:d[1]==terminal_args[2]).min()"
      ],
      "metadata": {
        "id": "TkmrA_3IPv3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Question 3\n",
        "\n",
        "Create  dataset  (text  file)  of bank transactions. Fields in file are ‘Bank ID’, ‘Account Number’, ‘Transaction Date’, ‘Transaction Type’ (credit or debit), ‘Transaction Amount’.Date format is dd-mm-yyyy.\n",
        "\n",
        "1. Count unique number of customers\n",
        "2. Count unique number of Bank ID\n",
        "3. Count unique number of customers per Bank ID\n",
        "4. Number of transactions for given Account Number\n",
        "5. Number of credit transactions for given Account Number in a given year"
      ],
      "metadata": {
        "id": "a-zpQegyuXbG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = spark.sparkContext.textFile('/content/drive/MyDrive/Colab Notebooks/ABD/DataSet/bank.txt')\n",
        "data.collect()"
      ],
      "metadata": {
        "id": "gzeQR9jZuu9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count unique number of customers\n",
        "\n",
        "data.map(lambda d: (d.split('\\t')[0],1)).reduceByKey(lambda a,b: a+b).collect()"
      ],
      "metadata": {
        "id": "8mM110EXwTHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count unique number of Bank ID\n",
        "\n",
        "data.map(lambda d: (d.split('\\t')[1],1)).reduceByKey(lambda a,b: a+b).collect()"
      ],
      "metadata": {
        "id": "9XGNCMSmxBHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count unique number of customers per Bank ID\n",
        "\n",
        "# data.map(lambda d: (d.split('\\t')[1],d.split('\\t')[0])).filter(lambda d: d[1] == 'CAN00123').distinct().collect()"
      ],
      "metadata": {
        "id": "IzJMmsR4xI-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data.map(lambda d: (d.split('\\t')[1],d.split('\\t')[0])).reduceByKey(lambda a,b: a+' '+b).collect()"
      ],
      "metadata": {
        "id": "VpfotVxnGDei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count unique number of customers per Bank ID\n",
        "\n",
        "data.map(lambda d: (d.split('\\t')[1],d.split('\\t')[0])).reduceByKey(lambda a,b: a+' '+b).map(lambda d: (d[0],{i for i in d[1].split(' ')})).collect()"
      ],
      "metadata": {
        "id": "YcSP_0pRQBEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of transactions for given Account Number\n",
        "\n",
        "# data.map(lambda d: (d.split('\\t')[3],d.split('\\t')[1])).reduceByKey(lambda a,b: a+' '+b).map(lambda d: (d[0],{i for i in d[1].split(' ')})).collect()\n",
        "data.map(lambda d: (d.split('\\t')[1],d.split('\\t')[3])).reduceByKey(lambda a,b: a+' '+b).collect()\n",
        "data.map(lambda d: (d.split('\\t')[1],d.split('\\t')[3])).reduceByKey(lambda a,b: a+' '+b).filter(lambda d: d[0] == '123876122').collect()"
      ],
      "metadata": {
        "id": "F7CaexiATUAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of credit transactions for given Account Number in a given year\n",
        "\n",
        "bankRdd.map(lambda d:((d.split('\\t')[1],\"On the year: \"+str(d.split('\\t')[2].split('-')[2])),1)).reduceByKey(lambda a,b:(a+b)).map(lambda d:(d[0],\"Number of transactions: \"+str(d[1]))).collect()"
      ],
      "metadata": {
        "id": "eTSEgMZKZmyX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}